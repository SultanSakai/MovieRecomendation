{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 parsing done!\n",
      "page 2 parsing done!\n",
      "page 3 parsing done!\n",
      "page 4 parsing done!\n",
      "page 5 parsing done!\n",
      "page 6 parsing done!\n",
      "page 7 parsing done!\n",
      "page 8 parsing done!\n",
      "page 9 parsing done!\n",
      "page 10 parsing done!\n",
      "page 11 parsing done!\n",
      "page 12 parsing done!\n",
      "page 13 parsing done!\n",
      "page 14 parsing done!\n",
      "page 15 parsing done!\n",
      "page 16 parsing done!\n",
      "page 17 parsing done!\n",
      "page 18 parsing done!\n",
      "page 19 parsing done!\n",
      "page 20 parsing done!\n",
      "page 21 parsing done!\n",
      "page 22 parsing done!\n",
      "page 23 parsing done!\n",
      "page 24 parsing done!\n",
      "page 25 parsing done!\n",
      "page 26 parsing done!\n",
      "page 27 parsing done!\n",
      "page 28 parsing done!\n",
      "page 29 parsing done!\n",
      "page 30 parsing done!\n",
      "page 31 parsing done!\n",
      "page 32 parsing done!\n",
      "page 33 parsing done!\n",
      "page 34 parsing done!\n",
      "page 35 parsing done!\n",
      "page 36 parsing done!\n",
      "page 37 parsing done!\n",
      "page 38 parsing done!\n",
      "page 39 parsing done!\n",
      "page 40 parsing done!\n",
      "page 41 parsing done!\n",
      "page 42 parsing done!\n",
      "page 43 parsing done!\n",
      "page 44 parsing done!\n",
      "page 45 parsing done!\n",
      "page 46 parsing done!\n",
      "page 47 parsing done!\n",
      "page 48 parsing done!\n",
      "page 49 parsing done!\n",
      "page 50 parsing done!\n",
      "page 51 parsing done!\n",
      "page 52 parsing done!\n",
      "page 53 parsing done!\n",
      "page 54 parsing done!\n",
      "page 55 parsing done!\n",
      "page 56 parsing done!\n",
      "page 57 parsing done!\n",
      "page 58 parsing done!\n",
      "page 59 parsing done!\n",
      "page 60 parsing done!\n",
      "page 61 parsing done!\n",
      "page 62 parsing done!\n",
      "page 63 parsing done!\n",
      "page 64 parsing done!\n",
      "page 65 parsing done!\n",
      "page 66 parsing done!\n",
      "page 67 parsing done!\n",
      "page 68 parsing done!\n",
      "page 69 parsing done!\n",
      "page 70 parsing done!\n",
      "page 71 parsing done!\n",
      "page 72 parsing done!\n",
      "page 73 parsing done!\n",
      "page 74 parsing done!\n",
      "page 75 parsing done!\n",
      "page 76 parsing done!\n",
      "page 77 parsing done!\n",
      "page 78 parsing done!\n",
      "page 79 parsing done!\n",
      "page 80 parsing done!\n",
      "page 81 parsing done!\n",
      "page 82 parsing done!\n",
      "page 83 parsing done!\n",
      "page 84 parsing done!\n",
      "page 85 parsing done!\n",
      "page 86 parsing done!\n",
      "page 87 parsing done!\n",
      "page 88 parsing done!\n",
      "page 89 parsing done!\n",
      "page 90 parsing done!\n",
      "page 91 parsing done!\n",
      "page 92 parsing done!\n",
      "page 93 parsing done!\n",
      "page 94 parsing done!\n",
      "page 95 parsing done!\n",
      "page 96 parsing done!\n",
      "page 97 parsing done!\n",
      "page 98 parsing done!\n",
      "page 99 parsing done!\n",
      "page 100 parsing done!\n",
      "page 101 parsing done!\n",
      "page 102 parsing done!\n",
      "page 103 parsing done!\n",
      "page 104 parsing done!\n",
      "page 105 parsing done!\n",
      "page 106 parsing done!\n",
      "page 107 parsing done!\n",
      "page 108 parsing done!\n",
      "page 109 parsing done!\n",
      "page 110 parsing done!\n",
      "page 111 parsing done!\n",
      "page 112 parsing done!\n",
      "page 113 parsing done!\n",
      "page 114 parsing done!\n",
      "page 115 parsing done!\n",
      "page 116 parsing done!\n",
      "page 117 parsing done!\n",
      "page 118 parsing done!\n",
      "page 119 parsing done!\n",
      "page 120 parsing done!\n",
      "page 121 parsing done!\n",
      "page 122 parsing done!\n",
      "page 123 parsing done!\n",
      "page 124 parsing done!\n",
      "page 125 parsing done!\n",
      "page 126 parsing done!\n",
      "page 127 parsing done!\n",
      "page 128 parsing done!\n",
      "page 129 parsing done!\n",
      "page 130 parsing done!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open(\"urls_for_parsing.json\", 'r', encoding='utf-8') as file:\n",
    "        results = json.load(file)\n",
    "except:\n",
    "    results = {}\n",
    "\n",
    "for i in range(1,131):\n",
    "    url = f\"https://kinogo.net/page/{i}/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        links = soup.find_all(\"a\", class_=\"item__title flex-grow-1\")\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            text = link.get_text()\n",
    "            results[text] = \"https://kinogo.net\"+href\n",
    "    print(f\"page {i} parsing done!\")\n",
    "\n",
    "with open(\"urls_for_parsing.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка фильмов: 100%|██████████| 1287/1287 [07:40<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно сохранены в файл: films.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_data(text, name, desription):\n",
    "    data = {}\n",
    "    data[\"name\"] = name\n",
    "    data[\"year\"] = re.search(r\"Год выпуска:\\s*(\\d+)\", text).group(1) if re.search(r\"Год выпуска:\\s*(\\d+)\", text) else \"\"\n",
    "    data[\"country\"] = re.search(r\"Страна:\\s*([\\w, ]+)\", text).group(1) if re.search(r\"Страна:\\s*([\\w, ]+)\", text) else \"\"\n",
    "    data[\"genre\"] = re.search(r\"Жанр:\\s*([\\w, ]+)\", text).group(1) if re.search(r\"Жанр:\\s*([\\w, ]+)\", text) else \"\"\n",
    "    \n",
    "    duration_match = re.search(r\"Продолжительность:\\s*(\\d+)\", text)\n",
    "    data[\"duration\"] = int(duration_match.group(1)) if duration_match else \"\"\n",
    "    \n",
    "    data[\"director\"] = re.search(r\"Режиссер:\\s*([\\w .]+)\", text).group(1) if re.search(r\"Режиссер:\\s*([\\w .]+)\", text) else \"\"\n",
    "    data[\"cast\"] = re.search(r\"Актёры:\\s*(.+)\", text).group(1).strip() if re.search(r\"Актёры:\\s*(.+)\", text) else \"\"\n",
    "    data[\"description\"] = desription\n",
    "    return data\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "all_films_data = []\n",
    "\n",
    "for name in tqdm(results, desc=\"Обработка фильмов\"):\n",
    "    url = results[name]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    ul = soup.find(\"ul\", class_=\"item__list d-flex fd-column\")\n",
    "    str_ul = str(ul)\n",
    "    str_ul = re.sub(r\"<.*?>\", \"\", str_ul)\n",
    "    \n",
    "    div = soup.find(\"div\", class_=\"page__text full-text clearfix\")\n",
    "    str_div = str(div)\n",
    "    str_div = re.sub(r\"<.*?>\", \"\", str_div)\n",
    "\n",
    "    film_data = extract_data(str_ul, name, str_div)\n",
    "\n",
    "    all_films_data.append(film_data)\n",
    "\n",
    "\n",
    "csv_file = \"films.csv\"\n",
    "with open(csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"name\",\"year\", \"country\", \"genre\", \"duration\", \"director\", \"cast\", \"description\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_films_data)\n",
    "\n",
    "print(f\"Данные успешно сохранены в файл: {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
