{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 parsing done!\n",
      "page 2 parsing done!\n",
      "page 3 parsing done!\n",
      "page 4 parsing done!\n",
      "page 5 parsing done!\n",
      "page 6 parsing done!\n",
      "page 7 parsing done!\n",
      "page 8 parsing done!\n",
      "page 9 parsing done!\n",
      "page 10 parsing done!\n",
      "page 11 parsing done!\n",
      "page 12 parsing done!\n",
      "page 13 parsing done!\n",
      "page 14 parsing done!\n",
      "page 15 parsing done!\n",
      "page 16 parsing done!\n",
      "page 17 parsing done!\n",
      "page 18 parsing done!\n",
      "page 19 parsing done!\n",
      "page 20 parsing done!\n",
      "page 21 parsing done!\n",
      "page 22 parsing done!\n",
      "page 23 parsing done!\n",
      "page 24 parsing done!\n",
      "page 25 parsing done!\n",
      "page 26 parsing done!\n",
      "page 27 parsing done!\n",
      "page 28 parsing done!\n",
      "page 29 parsing done!\n",
      "page 30 parsing done!\n",
      "page 31 parsing done!\n",
      "page 32 parsing done!\n",
      "page 33 parsing done!\n",
      "page 34 parsing done!\n",
      "page 35 parsing done!\n",
      "page 36 parsing done!\n",
      "page 37 parsing done!\n",
      "page 38 parsing done!\n",
      "page 39 parsing done!\n",
      "page 40 parsing done!\n",
      "page 41 parsing done!\n",
      "page 42 parsing done!\n",
      "page 43 parsing done!\n",
      "page 44 parsing done!\n",
      "page 45 parsing done!\n",
      "page 46 parsing done!\n",
      "page 47 parsing done!\n",
      "page 48 parsing done!\n",
      "page 49 parsing done!\n",
      "page 50 parsing done!\n",
      "page 51 parsing done!\n",
      "page 52 parsing done!\n",
      "page 53 parsing done!\n",
      "page 54 parsing done!\n",
      "page 55 parsing done!\n",
      "page 56 parsing done!\n",
      "page 57 parsing done!\n",
      "page 58 parsing done!\n",
      "page 59 parsing done!\n",
      "page 60 parsing done!\n",
      "page 61 parsing done!\n",
      "page 62 parsing done!\n",
      "page 63 parsing done!\n",
      "page 64 parsing done!\n",
      "page 65 parsing done!\n",
      "page 66 parsing done!\n",
      "page 67 parsing done!\n",
      "page 68 parsing done!\n",
      "page 69 parsing done!\n",
      "page 70 parsing done!\n",
      "page 71 parsing done!\n",
      "page 72 parsing done!\n",
      "page 73 parsing done!\n",
      "page 74 parsing done!\n",
      "page 75 parsing done!\n",
      "page 76 parsing done!\n",
      "page 77 parsing done!\n",
      "page 78 parsing done!\n",
      "page 79 parsing done!\n",
      "page 80 parsing done!\n",
      "page 81 parsing done!\n",
      "page 82 parsing done!\n",
      "page 83 parsing done!\n",
      "page 84 parsing done!\n",
      "page 85 parsing done!\n",
      "page 86 parsing done!\n",
      "page 87 parsing done!\n",
      "page 88 parsing done!\n",
      "page 89 parsing done!\n",
      "page 90 parsing done!\n",
      "page 91 parsing done!\n",
      "page 92 parsing done!\n",
      "page 93 parsing done!\n",
      "page 94 parsing done!\n",
      "page 95 parsing done!\n",
      "page 96 parsing done!\n",
      "page 97 parsing done!\n",
      "page 98 parsing done!\n",
      "page 99 parsing done!\n",
      "page 100 parsing done!\n",
      "page 101 parsing done!\n",
      "page 102 parsing done!\n",
      "page 103 parsing done!\n",
      "page 104 parsing done!\n",
      "page 105 parsing done!\n",
      "page 106 parsing done!\n",
      "page 107 parsing done!\n",
      "page 108 parsing done!\n",
      "page 109 parsing done!\n",
      "page 110 parsing done!\n",
      "page 111 parsing done!\n",
      "page 112 parsing done!\n",
      "page 113 parsing done!\n",
      "page 114 parsing done!\n",
      "page 115 parsing done!\n",
      "page 116 parsing done!\n",
      "page 117 parsing done!\n",
      "page 118 parsing done!\n",
      "page 119 parsing done!\n",
      "page 120 parsing done!\n",
      "page 121 parsing done!\n",
      "page 122 parsing done!\n",
      "page 123 parsing done!\n",
      "page 124 parsing done!\n",
      "page 125 parsing done!\n",
      "page 126 parsing done!\n",
      "page 127 parsing done!\n",
      "page 128 parsing done!\n",
      "page 129 parsing done!\n",
      "page 130 parsing done!\n",
      "page 131 parsing done!\n",
      "page 132 parsing done!\n",
      "page 133 parsing done!\n",
      "page 134 parsing done!\n",
      "page 135 parsing done!\n",
      "page 136 parsing done!\n",
      "page 137 parsing done!\n",
      "page 138 parsing done!\n",
      "page 139 parsing done!\n",
      "page 140 parsing done!\n",
      "page 141 parsing done!\n",
      "page 142 parsing done!\n",
      "page 143 parsing done!\n",
      "page 144 parsing done!\n",
      "page 145 parsing done!\n",
      "page 146 parsing done!\n",
      "page 147 parsing done!\n",
      "page 148 parsing done!\n",
      "page 149 parsing done!\n",
      "page 150 parsing done!\n",
      "page 151 parsing done!\n",
      "page 152 parsing done!\n",
      "page 153 parsing done!\n",
      "page 154 parsing done!\n",
      "page 155 parsing done!\n",
      "page 156 parsing done!\n",
      "page 157 parsing done!\n",
      "page 158 parsing done!\n",
      "page 159 parsing done!\n",
      "page 160 parsing done!\n",
      "page 161 parsing done!\n",
      "page 162 parsing done!\n",
      "page 163 parsing done!\n",
      "page 164 parsing done!\n",
      "page 165 parsing done!\n",
      "page 166 parsing done!\n",
      "page 167 parsing done!\n",
      "page 168 parsing done!\n",
      "page 169 parsing done!\n",
      "page 170 parsing done!\n",
      "page 171 parsing done!\n",
      "page 172 parsing done!\n",
      "page 173 parsing done!\n",
      "page 174 parsing done!\n",
      "page 175 parsing done!\n",
      "page 176 parsing done!\n",
      "page 177 parsing done!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open(\"urls_for_parsing_anime.json\", 'r', encoding='utf-8') as file:\n",
    "        results = json.load(file)\n",
    "except:\n",
    "    results = {}\n",
    "\n",
    "for i in range(1,178):\n",
    "    url = f\"https://anilibrias.ru/page/{i}/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        links = soup.find_all(\"a\", class_=\"kino-h\")\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            text = link.get_text()\n",
    "            results[text] = href\n",
    "    print(f\"page {i} parsing done!\")\n",
    "\n",
    "with open(\"urls_for_parsing_anime.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка фильмов: 100%|██████████| 1287/1287 [07:40<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно сохранены в файл: films.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_data(text, name, desription):\n",
    "    data = {}\n",
    "    data[\"name\"] = name\n",
    "    data[\"year\"] = re.search(r\"Год выпуска:\\s*(\\d+)\", text).group(1) if re.search(r\"Год выпуска:\\s*(\\d+)\", text) else \"\"\n",
    "    data[\"country\"] = re.search(r\"Страна:\\s*([\\w, ]+)\", text).group(1) if re.search(r\"Страна:\\s*([\\w, ]+)\", text) else \"\"\n",
    "    data[\"genre\"] = re.search(r\"Жанр:\\s*([\\w, ]+)\", text).group(1) if re.search(r\"Жанр:\\s*([\\w, ]+)\", text) else \"\"\n",
    "    \n",
    "    duration_match = re.search(r\"Продолжительность:\\s*(\\d+)\", text)\n",
    "    data[\"duration\"] = int(duration_match.group(1)) if duration_match else \"\"\n",
    "    \n",
    "    data[\"director\"] = re.search(r\"Режиссер:\\s*([\\w .]+)\", text).group(1) if re.search(r\"Режиссер:\\s*([\\w .]+)\", text) else \"\"\n",
    "    data[\"cast\"] = re.search(r\"Актёры:\\s*(.+)\", text).group(1).strip() if re.search(r\"Актёры:\\s*(.+)\", text) else \"\"\n",
    "    data[\"description\"] = desription\n",
    "    return data\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "all_films_data = []\n",
    "\n",
    "for name in tqdm(results, desc=\"Обработка фильмов\"):\n",
    "    url = results[name]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    ul = soup.find(\"ul\", class_=\"item__list d-flex fd-column\")\n",
    "    str_ul = str(ul)\n",
    "    str_ul = re.sub(r\"<.*?>\", \"\", str_ul)\n",
    "    \n",
    "    div = soup.find(\"div\", class_=\"page__text full-text clearfix\")\n",
    "    str_div = str(div)\n",
    "    str_div = re.sub(r\"<.*?>\", \"\", str_div)\n",
    "\n",
    "    film_data = extract_data(str_ul, name, str_div)\n",
    "\n",
    "    all_films_data.append(film_data)\n",
    "\n",
    "\n",
    "csv_file = \"films.csv\"\n",
    "with open(csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"name\",\"year\", \"country\", \"genre\", \"duration\", \"director\", \"cast\", \"description\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_films_data)\n",
    "\n",
    "print(f\"Данные успешно сохранены в файл: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка фильмов: 100%|██████████| 2123/2123 [47:39<00:00,  1.35s/it] \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'posters/posters_anime.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m         img_rel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     posters[name] \u001b[38;5;241m=\u001b[39m img_rel\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposters/posters_anime.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     35\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(posters, file, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/movie/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'posters/posters_anime.json'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    with open(\"urls_for_parsing_anime.json\", 'r', encoding='utf-8') as file:\n",
    "        results = json.load(file)\n",
    "except:\n",
    "    results = {}\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "posters={}\n",
    "\n",
    "for name in tqdm(results, desc=\"Обработка фильмов\"):\n",
    "    url = results[name]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    a = soup.find(\"a\", class_=\"highslide\")\n",
    "\n",
    "    try:\n",
    "        img_rel = a.get(\"href\")\n",
    "    except:\n",
    "        img_rel = \"\"\n",
    "\n",
    "    posters[name] = img_rel\n",
    "\n",
    "with open(\"posters/posters_anime.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(posters, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"posters_anime.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(posters, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
